```{r, include=FALSE}
url <- "https://scienceverse.github.io/talks/2025-papercheck-intro/"
subtitle <- sprintf("[%s](%s)", url, url)
# make QR code
qrcode::qr_code(url) |> qrcode::generate_svg("images/qrcode.svg")
```
---
title: "Papercheck"
subtitle: "`r subtitle`"
author: "Lisa DeBruine \n\n ![](images/qrcode.svg){width=300}"
format: 
  revealjs:
    logo: images/papercheck.png
    theme: [default, style.scss]
    transition: none
    transition-speed: fast
---

# Abstract

```{r, include = FALSE}
library(papercheck)
library(tidyverse)

theme_set(theme_minimal(13))

knitr::opts_chunk$set(echo = TRUE)
```

::: {style="font-size: smaller"}
In this talk, I will introduce Papercheck, a new tool that leverages text search, code, and large language models to extract and supplement information from scientific documents (including manuscripts, submitted or published articles, or preregistration documents) and provides automated suggestions for improvement. 
 
Inspired by practices in software development, where automated checks (e.g., CRAN checks for R packages) are used to identify issues before release, Papercheck aims to screen scientific manuscripts to identify potential issues or areas for improvement and guide researchers in adopting best practices. It can also assist with processing large numbers of papers for metascientific enquiry.
:::

# The Problem

## Best Practices are Rapidly Evolving

::: {.notes}
- share data, code, and materials
- correctly apply statistical methods
- declare conflicts of interests 
- transparent author contributions
- appropriately cite papers; acknowledge retractions
- avoid overclaiming and inappropriate use of causal language
:::

::: {layout="[[2,1,1], [1,2,1]]"}

![](images/os-foster.jpg)

![](images/os-liber.jpg)

![](images/os-opus.png)

![](images/os-unesco.png)

![](images/os-4pillars.jpg)

![](images/os-umbrella.png)
:::


## Un-FAIR Meta-Data

:::: columns
::: {.column width="30%"}

::: {layout="[[1], [1]]"}
![](images/FAIR.png)

![](images/pdf-die.png)
:::

:::
::: {.column width="70%"}

- All research outputs should be FAIR
- PDFs are where data goes to die
- Meta-data use cases:
    - facilitating meta-analyses
    - improving the re-use of reliable measures
    - meta-scientific research

:::
::::


# Solutions

## Checklists?

:::: columns
::: {.column width="50%"}

Reporting guidelines, such as [CONSORT](https://www.goodreports.org/reporting-checklists/consort/), [PRISMA](https://prisma.shinyapps.io/checklist/), and [JARS](https://apastyle.apa.org/jars) often provide extensive checklists.

<style>
.checklist li::before {
  background-image: url('images/checkbox.svg');
}
</style>

::: {.checklist}
- Time-consuming
- Requires expertise
- Can be vague
- Who checks the checklist?
:::

:::

::: {.column width="50%"}

::: {.panel-tabset}

### JARS-1

![](images/JARS-quant1.png)

### 2

![](images/JARS-quant2.png)

### 3

![](images/JARS-quant3.png)

:::

:::
::::

## Automated Checks

::: {.checklist}
- Time-efficient
- Requires less expertise
- Reproducible
- Generates machine-readable metadata
:::

## Automation Strategies

[Grobid](grobid.readthedocs.io): A machine learning software for extracting structured information from scholarly documents

And then...

::: {layout="[[1,1,1]]"}
![Text Search](images/regex.png)

![Code](images/code.png)


![LLM](images/llm.jpg)
:::


# R Package

![](images/papercheck.png)

## Paper Import

```{r, eval = FALSE}
file <- "papers/debruine-fret.pdf"
xml <- pdf2grobid(file, 
                  consolidateCitations = TRUE, 
                  consolidateHeader = TRUE)
paper <- read(xml)
```

```{r, echo = FALSE}
paper <- read("papers/debruine-fret.xml")
paper
```




## Batch Import

```{r}
papers <- read("papers")
```

:::: columns
::: {.column width="25%"}
```{r, echo = FALSE}
papers[[1]]
```
:::
::: {.column width="25%"}
```{r, echo = FALSE}
papers[[2]]
```
:::
::: {.column width="25%"}
```{r, echo = FALSE}
papers[[3]]
```
:::
::: {.column width="25%"}
```{r, echo = FALSE}
papers[[4]]
```
:::
::::


## Text Search

```{r, eval = FALSE}
search_text(paper, "Canadian")
```
::: {.module}

```{r, echo = FALSE}
search_text(paper, "Canadian") |> knitr::kable()
```

:::

## Regex Text Search

```{r, eval = FALSE}
search_text(paper, "\\b\\S+\\s*=\\s*\\S+\\b", return = "match")
```

::: {.module}

```{r, echo = FALSE}
search_text(paper, "\\b\\S+\\s*=\\s*\\S+\\b", return = "match") |> head(10) |> knitr::kable()
```

:::

## LLM

```{r, eval = FALSE}
query <- 'How many subjects were in the studies in total? 
Return your answer in JSON format giving the total and 
any subgroupings by gender, e.g.:
{"total": 100, men": 42, "women": 58}, 
Only return valid JSON, no notes.'

llm_subjects <- papers |> 
  search_text("\\d+", section = "method") |>
  search_text(return = "section") |>
  llm(query)

llm_subjects |> json_expand()
```

::: {.module}

```{r, echo = FALSE}

# llm_subjects[, c("id", "answer")] |> json_expand() |> dput()

data.frame(
  id = c("debruine-child", "debruine-fret", 
         "debruine-sex", "debruine-tnl"), 
  answer = c(
    "{\"total\": 71, \"men\": 32, \"women\": 39}", 
    "{\"total\": 48, \"men\": 24, \"women\": 24}", 
    "{\"total\": 136, \"men\": 86, \"women\": 50}", 
    "{\"total\": 144, \"men\": 66, \"women\": 78}"), 
  total = c(71L, 48L, 136L, 144L), 
  men = c(32L, 24L, 86L, 66L), 
  women = c(39L, 24L, 50L, 78L)
) |> knitr::kable()
```

:::

## OSF Functions

```{r, eval = FALSE}
# find all OSF links in the papers
links <- osf_links(psychsci)

# get info about a link and its children
osf <- osf_retrieve(links$text[1], recursive = TRUE)

# download all files
osf_file_download(links$text[1])
```

::: {.module}
```{r, echo = FALSE}
#osf[, c(2:3, 5:6, 10, 13:14)] |> dput()

data.frame(
  osf_id = c("e2aks", "7jh5v", "pj4e8", 
             "553e58658c5e4a219919a629", 
              "553e58658c5e4a219919a62a", 
              "553e58658c5e4a219919a62c", 
              "553e58658c5e4a219919a628", 
              "553e58658c5e4a219919a62b", 
              "553e7e168c5e4a21991a4dac"), 
  name = c("Action-specific disruption of perceptual confidence", 
  "Data", "Analysis scripts", "allData_orientation.txt", "allData_contrast_M1.txt", "Mratio_contrast_M1.txt", "Mratio_all.txt", "allData_contrast_PMC.txt", "tms_analysis.R"), 
  osf_type = c("nodes", "nodes", "nodes", "files", "files", "files", "files", "files", "files"), 
  public = c(TRUE, TRUE, TRUE, NA, NA, NA, NA, NA, NA), 
  parent = c(NA, "e2aks", "e2aks", "7jh5v", "7jh5v", "7jh5v", "7jh5v", "7jh5v", "pj4e8"), 
  size = c(NA, NA, NA, 1096113L, 691560L, 5508L, 15326L, 712804L, 5670L), 
  downloads = c(NA, NA, NA, 41L, 68L, 31L, 34L, 59L, 66L
)) |>knitr::kable()
```
::: 

## Modules

```{r}
module_list()
```

## Modules: Effect Sizes

::: {.notes}
A JARS guideline that can be automatically checked is whether people report effect sizes alongside their test result. Each test, like a t-test or F-test, should include the corresponding effect size, like a Cohen’s d, or partial eta-squared. Based on a text search that uses regular expressions (regex), we can identify t-tests and F-tests that are not followed by an effect size, and warn researchers accordingly.
:::

```{r}
mod <- module_run(
  paper = psychsci,
  module = "effect_size"
)
```

::: {.module}

```{r, echo = FALSE}
mod$summary |> head(10) |> knitr::kable()
```
:::

## Modules: Effect Sizes

```{r, eval = FALSE}
mod <- module_run(
  paper = psychsci,
  module = "effect_size"
)
```

```{r, echo = FALSE}
info_table(psychsci, "accepted") |>
  left_join(mod$summary, by = "id") |>
  mutate(accepted = as_date(accepted),
         year = year(accepted)) |>
  filter(!is.na(year)) |>
  pivot_longer(cols = ttests_n:Ftests_without_es) |>
  separate(name, c("test", "stat"), extra = "drop") |>
  pivot_wider(names_from = stat) |>
  summarise(n = sum(n), with = sum(with), .by = c("year", "test")) |>
  ggplot(aes(x = year, y = with/n, colour = test)) +
  stat_summary() +
  geom_line() +
  # geom_text(aes(label = n), y = 0) +
  scale_x_continuous(breaks = 2000:2025) +
  scale_color_manual(values = c("darkorange", "darkgreen")) +
  guides(
    colour = guide_legend(position = "inside")
  ) +
  labs(x = "",
       y = "Percent of tests with effect sizes", color = "") +
  theme(legend.position.inside = c(.8, .2))


```


## Modules: StatCheck

::: {.notes}
 
:::

This module uses the {[statcheck](https://michelenuijten.shinyapps.io/statcheck-web/)} package to check the consistency of p-values and test statistics. 

::: {.module}

```{r, eval = FALSE}
module_run(psychsci[6:10], "statcheck")
```

```{r, echo = FALSE, results='asis'}
sc <- module_run(psychsci[6:10], "statcheck")
sc$table <- dplyr::select(sc$table, reported_p:apa_factor)
sc
```

:::

:::{.smaller}
It currently only works for APA-formatted stats, but we are working on an extended version.
:::

## Modules: Exact P-Values

::: {.notes}
Those of us of a certain age remember looking up p-values from tables of test statistics and degrees of freedom, leading to the common practice of reporting p-values inexactly, such as “p < .05”. However, modern computational techniques allow us to report p-values exactly, and this practice helps us to check papers for inconsistencies (e.g., Statcheck) and to reuse findings in meta-analysis. 
:::

This module scans the text for all p-values and flags those reported inexactly, such as p < .01, p < .10, or p = n.s. 

::: {.module}

```{r, results='asis'}
module_run(psychsci[1:10], "exact_p")
```

:::

## Modules: Marginal Significance

::: {.notes}
Researchers commonly interpret statistically non-significant test results just above 0.05 as 'marginally significant'. This practice, beyond being formally incorrect, inflates the Type 1 error rate of scientific claims when such results are interpreted as supporting the hypothesis, just as if they were statistically significant. For example, in the Reproducibility Project: Psychology where 100 studies were replicated, 97 of the original studies were interpreted as supporting the hypothesis, even though four studies had p values just above the alpha level (0.0508, 0.0514, 0.0516, and 0.0567). None of these studies replicated. 
:::

This module searches the text for phrases such as “marginally significant” or “borderline significance” and flags them.

::: {.module}

```{r, results='asis'}
module_run(psychsci[8], "marginal")
```

:::

## Modules: Inaccessible Resources

::: {.notes}
While the sharing of data, code and materials underlying scientific claims is increasing, with many research funders and journals encouraging or requiring this practice, it is a frequent frustration that the references to such resources are not actually findable or accessible. One common problem with the popular sharing platform OSF is that projects can be public or private, and researchers can easily accidentally fail to make linked resources public.

Indeed, in a scan of 100 unique OSF links from papers published in Psychological Science, we found that 3% led to inaccessible projects. 

:::

This module scans text for references to OSF projects and checks their status, flagging users if any of the links are either broken or lead to inaccessible private projects. 

::: {.module}

```{r, eval=FALSE}
module_run(psychsci[1:10], "osf_check")
```

```{r, echo=FALSE}
#osf <- module_run(psychsci[1:10], "osf_check")
o <- data.frame(
  id = c("0956797613520608", "0956797614522816", 
         "0956797614527830", "0956797614557697", 
         "0956797614560771", "0956797614566469", 
         "0956797615569001", "0956797615569889", 
         "0956797615583071", "0956797615588467"
  ), 
  osf.open = c(0, 0, 0, 2, 0, 0, 1, 0, 2, 0),
  osf.closed = c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0),
  osf.invalid = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
)

knitr::kable(o)
```


:::

## Modules: Reference Consistency

This modules checks for missing references or citations.

::: {.module}

```{r, results='asis'}
module_run(papers[[1]], "ref_consistency")
```

:::

## Modules: Retracted Papers

::: {.notes}
More than 10,000 scientific papers were retracted in 2023 (https://www.nature.com/articles/d41586-023-03974-8), for various reasons including honest error, duplicate publication, and misconduct. Retraction Watch (https://retractionwatch.com) keeps a database of retracted papers along with information about the reason for the retraction, which is also publicly available from CrossRef. This module includes the sentences in which a retracted paper was cited so that researchers can more easily check if the citation acknowledged the retracted status of the cited paper. 
:::

This module searches the [RetractionWatch](https://retractionwatch.com) database for all cited references in a paper and flags those that have been retracted.  

::: {.module}

```{r}
mod <- module_run(psychsci, "retractionwatch")

mod$summary |> dplyr::filter(rw_Correction + rw_Retraction > 0)
```

:::


# Promoting Adoption

![Center for Open Science](images/cos_triangle_text.webp)

## Workflows

::: {layout="[[1,1,1,1]]"}
![Individual](images/individual.svg)

![Automated](images/arxiv.jpg)

![Meta-Science](images/metascience.png)

![Systemic](images/systematic.png)
:::

## Caveats

:::: {.columns}

::: {.column width="30%"}

<!-- icon credit WEBTECHOPS LLP -->

![](images/caveat.png)

:::
::: {.column width="30%"}

* Validation
* Sustainability
* Inappropriate Use

:::
::::

# Thank You!

![](images/papercheck.png){width=120 style="vertical-align:middle;"} [papercheck](https://scienceverse.github.io/papercheck/) - download the package or submit issues

![](images/verisci.png){width=120 style="vertical-align:middle;"} [VeriSci](https://github.com/VeriSci/) - join a community to create or test modules

![](images/rainbow.png){width=120 style="vertical-align:middle;"} [@debruine](https://debruine.github.io) - see what else I'm up to

